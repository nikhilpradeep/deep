{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**1.  Object Classification**-\n",
    "Tells you what the “main subject” of the image is\n",
    "\n",
    "**2. Object Localization**- Predict and draw bounding boxes around on object in an image\n",
    "\n",
    "**3. Object Detection**- Find multiple objects, classify them, and locate where they are in the image.\n",
    "\n",
    "![](image/intro.jpeg)\n",
    "\n",
    "**Why it is difficult**\n",
    "\n",
    "1. Can have varying number of objects in an image and we do not know ahead of time how many we would expect\n",
    "in an image.\n",
    "2. Choosing a right crop is not a trivial task as we may encouter any number of images which :\n",
    "    1. can be at any place.\n",
    "    2. can be of any aspect ration.\n",
    "    3. can be of any size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General object detection framework\n",
    "\n",
    "Typically, there are three steps in an object detection framework. \n",
    "1. **Object localisation component** \n",
    "<br>\n",
    "A model or algorithm is used to generate regions of interest or region proposals. These region proposals are a large set of bounding boxes spanning the full image.\n",
    "\n",
    "Some of the famous approaches:\n",
    "* **Selective Search**  - A clustering based approach which attempts to group pixels and generate proposals based on the generated clusters.\n",
    "* **Region Proposal using Deep Learning Model (Features extracted from the image to generate regions)** - Based on the features from a deep learning model\n",
    "* **Brute Force** - Similar to a sliding window that is applied to the image, over several ratios and scales. These regions are generated automatically, without taking into account the image features.\n",
    "\n",
    "\n",
    "![](image/anchor_boxes.PNG)\n",
    "\n",
    "\n",
    "**Things to note:**\n",
    "* Trade-off that is made with region proposal generation is the number of regions vs. the computational complexity.\n",
    "* Use problem specific information to reduce the number of ROI’s (e.g. pedestrian typically have a ratio of approximately 1.5, so it is not useful to generate ROI’s with a ratio of 0.25).\n",
    "![](image/ped_car.JPG)\n",
    "\n",
    "\n",
    "2. **Object classification component** \n",
    "<br>\n",
    "In the second step, visual features are extracted for each of the bounding boxes, they are evaluated and it is determined whether and which objects are present in the proposals based on visual features.\n",
    "\n",
    "**Some of the famous approaches:**\n",
    "* Use pretrained image classification models to extract visual features\n",
    "* Traditional Computer Vision (filter based approached, histogram methods, etc.)\n",
    "\n",
    "![](image/cnn_layer.PNG)\n",
    "![](image/feature_map.PNG)\n",
    "\n",
    "3. **Non maximum suppression**\n",
    "<br>\n",
    "In the final post-processing step, reduce the number of detections in a frame to the actual number of objects present to make sure overlapping boxes are combined into a single bounding box.\n",
    "<br>\n",
    "NMS techniques are typically standard across the different detection frameworks, but it is an important step that might require hyperparameter tweaking based on the scenario.\n",
    "\n",
    "Predicted\n",
    "![](image/NMS_1.svg)\n",
    "\n",
    "Desired\n",
    "![](image/NMS_2.svg)\n",
    "\n",
    "**Evaluation**\n",
    "<br>\n",
    "Evaluation metric used:\n",
    "*  Mean Average Precision (mAP or  mAP@0.5 or mAP@0.25) -\n",
    "    *  It is a number from 0 to 100 and higher values are typically better\n",
    "    \n",
    "Steps to calculate mAP\n",
    "1. Predict bounding box scores(likelihood of the box containing an object).\n",
    "2. Based on the predictions a precision-recall curve (PR curve) is computed for each class by varying the score threshold.\n",
    "3. First the AP is computed for each class, which is the area under the PR curve and then averaged over the different classes. The end result is the mAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box Representation\n",
    "\n",
    "Bounding box is represented using : \n",
    "x_min , y_min , x_max , y_max\n",
    "![](image/bounding_1.PNG)\n",
    "\n",
    "But pixel values are next to useless if we don't know the actual dimensions of the image. A better way would be to represent all coordinates is in their fractional form.\n",
    "![](image/bounding_2.PNG)\n",
    "\n",
    "1. From boundary coordinates to centre size coordinates \n",
    "<br>\n",
    " Function- **xy_to_cxcy** \n",
    "<br>\n",
    "x_min,y_min,x_max,y_max -> c_x,c_y,w,h\n",
    "\n",
    "2. From centre size coordinates to bounding box coordinates \n",
    "Function - **cxcy_to_xy**\n",
    "<br>\n",
    "c_x , c_y , w , h -> x_min,y_min,x_max,y_max \n",
    "\n",
    "![](image/bounding_3.PNG)\n",
    "\n",
    "3. Offset from bounding box (used in the loss function) \n",
    "Function - **cxcy_to_gcxgcy** \n",
    "<br>\n",
    "    * **g_c_x ,g_c_y** -find the offset with respect to the prior box, and scale by the size of the prior box.\n",
    "    * **g_w , g_h** - scale by the size of the prior box, and convert to the log-space. \n",
    "    \n",
    "4. Decoding the predicted offset to centre size coordinates \n",
    "<br>\n",
    "Function - gcxgcy_to_cxcy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOU (Jaccard Index)\n",
    " \n",
    "How well the one box matches the the other box we can compute the IOU (or intersection-over-union, also known as the Jaccard index) between the two bounding boxes.\n",
    "\n",
    "Steps to Calculate:\n",
    "1. Find Intersection\n",
    "2. Find Union\n",
    "\n",
    "Jaccard Overlap = Intersection / Union\n",
    "\n",
    "![](image/IOU.PNG)\n",
    "\n",
    "Check out the excel sheet for the calculations\n",
    "![](image/IOU_excel.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD\n",
    "**2016 - Google**\n",
    "\n",
    "![](image/SSD.PNG)\n",
    "\n",
    "Single Shot Multibox Detector\n",
    "\n",
    "1. **Single Shot**:In a single forward pass of the network,the task of object localization and object classification are done.\n",
    "2. **Multibox**-Name of a technique for bounding box regression developed earlier by Szegedy et .al\n",
    "3. **Detector**-The network does the job of object detector and classifies those detected objects.\n",
    "\n",
    "VGG-16 \n",
    "![](image/vgg_16.PNG)\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11\n",
    "\n",
    "https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4\n",
    "\n",
    "https://lilianweng.github.io/lil-log/\n",
    "\n",
    "\n",
    "**Modification in Vgg Network:**\n",
    "\n",
    "![](image/cnn_layer.PNG)\n",
    "\n",
    "1. ** Input Image**  \n",
    "300 x300  instead of 224 x 224\n",
    "<br>\n",
    "2. ** 3rd Layer ** \n",
    "Ceil Mode\n",
    "Significant if the dimensions of the preceding feature map are odd and not even.In this case we get the input as 75 x75 which is halved to to 38, 38 instead of an inconvenient 37, 37.\n",
    "<br>\n",
    "3. **Max Pooling** \n",
    "5th pooling layer -\n",
    "From a 2, 2 kernel and 2 stride to a 3, 3 kernel and 1 stride and 1 padding. The effect this has is it no longer halves the dimensions of the feature map from the preceding convolutional layer.\n",
    "<br>\n",
    "4. ** Linear Layer **\n",
    "We will toss fc8  which is the classification layer.\n",
    " Rework(using decimate) fc6 and fc7 into convolutional layers conv6 and conv7. \n",
    "\n",
    "Rework Strategy-Reparameterize a fully connected layer into a convolutional layer\n",
    "<br>\n",
    "**Things to note**\n",
    "    * An image of size H, W with I input channels, a fully connected layer of output size N is equivalent to a convolutional layer with kernel size equal to the image size H, W and N output channels.\n",
    "    * fc6 with a flattened input size of 7 * 7 * 512 and an output size of 4096 has parameters of dimensions 4096, 7 * 7 * 512. The equivalent convolutional layer conv6 has a 7, 7 kernel size and 4096 output channels, with reshaped parameters of dimensions 4096, 7, 7, 512\n",
    "    * fc7 with an input size of 4096 (i.e. the output size of fc6) and an output size 4096 has parameters of dimensions 4096, 4096. The input could be considered as a 1, 1 image with 4096 input channels. The equivalent convolutional layer conv7 has a 1, 1 kernel size and 4096 output channels, with reshaped parameters of dimensions 4096, 1, 1, 4096.\n",
    "    * These filters are numerous and large – and computationally expensive.Hence opt to reduce both their number and the size of each filter by subsampling parameters.\n",
    "    fc6 - 1024 filters of size 3 x 3\n",
    "    fc7 - 1024 filters of size 1 x 1\n",
    "**Auxiliary Connection**\n",
    "<br>\n",
    "Stacking some more convolutional layers on top of our base network\n",
    "These convolutions provide additional feature maps, each progressively smaller than the last.\n",
    "<br>\n",
    "Conv8_1 ,  Conv8_2 \n",
    "<br>\n",
    "Conv9_1 ,  Conv9_2\n",
    "<br>\n",
    "Conv10_1 ,Conv_10_2 \n",
    "<br>\n",
    "Conv11_1 ,Conv_11_2\n",
    "<br>\n",
    "Feature Map of Conv_8_2 , Conv9_2 , Conv_10_2 , Conv_11_2 will be used for Detection\n",
    "6. Multiple Output Feature Map used for Detection:  \n",
    "  1. Conv4_3   - 38 x 38 x 512\n",
    "  2. Conv_7     - 19 x 19 x 1024\n",
    "  3. Conv_8_2 - 10 x 10 x 512\n",
    "  4. Conv_9_2 -  5 x 5 x 256\n",
    "  5. Conv_10_2- 3 x 3 x 256\n",
    "  6. Conv_11_2- 1 x 1 x 256\n",
    "  \n",
    "\n",
    "![](image/anchor_boxes_count.PNG)\n",
    "\n",
    "**Prediction Convolution**\n",
    "<br>\n",
    "Two Covolution layer for each feature map for class prediction  and localization prediction.\n",
    "\n",
    "For each prior at each location on each feature map, we want to predict –\n",
    "1. the offsets (g_c_x, g_c_y, g_w, g_h) for a bounding box.\n",
    "2. a set of n_classes scores for the bounding box, where n_classes represents the total number of object types (including a background class).\n",
    "\n",
    "What we do:\n",
    "<br>\n",
    "We need two convolutional layers for each feature map \n",
    "1. A ** localization prediction convolutional layer** with a 3, 3 kernel evaluating at each location (i.e. with padding and stride of 1) with 4 filters for each prior present at the location.\n",
    "\n",
    "The 4 filters for a prior calculate the four encoded offsets (g_c_x, g_c_y, g_w, g_h) for the bounding box predicted from that prior.\n",
    "\n",
    "2. A **class prediction convolutional layer** with a 3, 3 kernel evaluating at each location (i.e. with padding and stride of 1) with n_classes filters for each prior present at the location.\n",
    "\n",
    "The n_classes filters for a prior calculate a set of n_classes scores for that prior.\n",
    "\n",
    "\n",
    "Let us take one output feature map and understand :\n",
    " \n",
    "Considering Conv_9_2 output feature map of size 5 x 5 x256\n",
    "\n",
    "Step1:\n",
    "1. For localization:\n",
    "    1. Convolution:\n",
    "    5 x 5 x 256 ->  3 x 3 x 24  [6(Anchor boxes) x 4(Offsets)]\n",
    "    2. Output of size = 5 x 5 x 24 \n",
    "    3. Resize to 150(5x5x6 ) x 4\n",
    "   \n",
    "2. For class prediction \n",
    "    1. Convolution:\n",
    "    5 x 5 x 256 ->  3 x 3 x  126  [ 6 (Anchor Boxes)x 21 [20(Class Labels) +1(Background)]]\n",
    "    2. Output of size = 5 x 5 x 126 \n",
    "    3. Resize to 150(5x5x6 ) x 21\n",
    "    \n",
    "    \n",
    " Similarly we do for all output feature maps and stack the results together .Thus the  Output from the Predicted Convolution module is the following:\n",
    " 1. locs = 8732 x 4\n",
    " 2. class scores = 8732 x 21\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiBox Loss\n",
    "\n",
    "The MultiBox loss, a loss function for object detection.\n",
    "\n",
    "This is a combination of:\n",
    "1. A localization loss for the predicted locations of the boxes \n",
    "2.  A confidence loss for the predicted class scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "![](image/map.PNG)\n",
    "\n",
    "SSD300\\* and SSD512\\* applies data augmentation for small objects to improve mAP.)\n",
    "\n",
    "Data:\n",
    "”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and VOC2012 trainval.\n",
    "”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues with One Shot Detectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance \n",
    "\n",
    "There is extreme foreground-background class imbalance problem in one-stage detector\n",
    "\n",
    "![](image/issue1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Focussed on Hard Examples\n",
    "\n",
    "Hard Samples-Those examples where the difference between the true label and the predicted label is large,thus resulting in higher loss.\n",
    "Easy Samples:Those examples where the difference between the true label and the predicted label is small,thus resulting in lower loss.\n",
    "\n",
    "Standard Cross Entropy Loss treats both(hard and easy samples) equally.Due to which these small loss values of easy samples can overwhelm the rare class.\n",
    "\n",
    "![](image/ce.PNG)\n",
    "\n",
    "![](image/ce_graph.PNG)\n",
    "\n",
    "    The loss from easy examples = 100000×0.1 = 10000\n",
    "    The loss from hard examples = 100×2.3 = 230\n",
    "    It is about 40× (10000 / 2.3 = 43.) bigger loss from easy examples.\n",
    "Thus, CE loss is not a good choice when there is extreme class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed vs Accuracy Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to solve:\n",
    "\n",
    "** CLASS IMBALANCE**\n",
    "\n",
    "1. **Sampling heuristics**\n",
    "<br>\n",
    "Using fixed foreground-to-background ratio (1:3)\n",
    "2. **Online hard example mining (OHEM)**  \n",
    "Select a small set of anchors (e.g., 256) for each minibatch\n",
    "3. **$\\alpha$ balanced Cross entropy Loss**\n",
    "<br>\n",
    "Add a weighting factor α for class 1 and 1 - α for class -1.\n",
    "![](image/alpha_ce.PNG)\n",
    "\n",
    "**$\\alpha$** is set by inverse class frequency or treated as a hyperparameter to set by cross validation.\n",
    "\n",
    "Or\n",
    "<br>\n",
    "**$\\alpha$** is implicitly implemented by selecting the foreground-to-background ratio of 1:3.\n",
    "<br>\n",
    "However,training procedure is still dominated by easily classified background examples\n",
    "\n",
    "**CLASS IMBALANCE + Not Focussed on Hard Examples** \n",
    "\n",
    "1. **Focal Loss**\n",
    "\n",
    "The loss function is reshaped to down-weight easy examples and thus focus training on hard negatives. A modulating factor (1-pt)^ γ is added to the cross entropy loss where γ is tested from [0,5] in the experiment\n",
    "![](image/fl.PNG)\n",
    "\n",
    "Proporties of Focal Loss:\n",
    "1. When an example is misclassified and **pt** is small, the modulating factor is near 1 and the loss is unaffected. As **pt →1**, the factor goes to 0 and the loss for well-classified examples is down-weighted.\n",
    "2. The focusing parameter $\\gamma$ smoothly adjusts the rate at which easy examples are down-weighted. When $\\gamma$ = 0, FL is equivalent to CE. When $\\gamma$ is increased, the effect of the modulating factor is likewise increased. ($\\gamma$=2 works best in experiment.)\n",
    "\n",
    "For instance, with $\\gamma$ = 2, an example classified with pt = 0.9 would have 100 lower loss compared with CE and with pt \u0019= 0.968 it would have 1000 lower loss. This in turn increases the importance of correcting misclassified examples.\n",
    "The loss is scaled down by at most 4× for pt ≤ \u00140.5 and γ = 2.\n",
    "\n",
    "\n",
    "**To handle Class Imbalance:**\n",
    "\n",
    "2. **$\\alpha$-Balanced FL**\n",
    "![](image/alpha_fl.PNG)\n",
    "\n",
    "  - γ: Focus more on hard examples.\n",
    "  - α: Offset class imbalance of number of examples. \n",
    "    \n",
    "**From Paper**\n",
    "   - α is added into the equation, which yields slightly improved accuracy over the one without α\n",
    "   - Using sigmoid activation function for computing p resulting in greater numerical stability.\n",
    "\n",
    "**CLASS IMBALANCE + Not Focussed on Hard Examples + Accuracy** \n",
    "\n",
    "1. RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "<br>\n",
    "https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25\n",
    "<br>\n",
    "https://machinethink.net/blog/object-detection/\n",
    "<br>\n",
    "https://towardsdatascience.com/going-deep-into-object-detection-bed442d92b34\n",
    "<br>\n",
    "https://d2l.ai/chapter_computer-vision/anchor.html\n",
    "<br>\n",
    "https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\n",
    "<br>\n",
    "https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359\n",
    "<br>\n",
    "https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH\n",
    "<br>\n",
    "https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html\n",
    "<br>\n",
    "https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4\n",
    "<br>\n",
    "https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\n",
    "<br>\n",
    "https://medium.com/@smallfishbigsea/notes-on-focal-loss-and-retinanet-9c614a2367c6\n",
    "<br>\n",
    "R-CNN\n",
    "http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
