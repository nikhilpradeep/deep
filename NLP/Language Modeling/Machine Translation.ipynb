{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation [MT]\n",
    "\n",
    "***Machine Translation (MT)*** is a technology that automatically translates text using termbases and advanced grammatical, syntactic and semantic analysis techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Trasalation\n",
    "\n",
    "1. Rule Based Machine Translation[RBMT]\n",
    "2. Statistical Machine Translation[SMT]\n",
    "3. Example based Machine Translation [EBMT]\n",
    "4. Neural Machine Translation[NMT]\n",
    "5. Hybrid Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Machine Translation\n",
    "\n",
    "Also called Knowledge Based Machine Translation,it works by parsing a source sentence to identify words and analyze its structure, and then converting it into the target language based on a manually determined set of rules encoded by linguistic experts. The rules attempt to define correspondences between the structure of the source language and that of the target language.\n",
    "\n",
    "**Disadvantages**\n",
    "1. Development of an RBMT system is time-consuming and labor-intensive and may take several years for one language pair.\n",
    "2. Human-encoded rules are unable to cover all possible linguistic phenomena and conflicts between existing rules may lead to poor translation quality when facing real-life texts.RBMT engines donâ€™t deal well with slang or metaphorical texts\n",
    "\n",
    "There are three types of RBMT systems:\n",
    "\n",
    "**1. Direct Method (DIctionary Based Machine Translation)**\n",
    "<br>\n",
    "Source language text are translated without passing through an intermediary representation. The words will be\n",
    "translated as a dictionary does word by word, usually without much correlation of meaning between them\n",
    "\n",
    "**2 Transfer Rule Based Machine Translation** \n",
    "<br>\n",
    "Morphological and syntactical analysis is the fundamental approaches in Transfer based systems. Here source\n",
    "language text is converted into less language specific representation and same level of abstraction is generated\n",
    "with the help of grammar rules and bilingual dictionaries\n",
    "\n",
    "Eg- [Mantra](https://mantra-rajbhasha.rb-aai.in/) -A transfer based tool by Indian Govt\n",
    "\n",
    "**3 Interlingual RBMT Systems**\n",
    "<br>\n",
    " In this method, source language is translated into an intermediary representation which does not depends on any languages. Target language is derived from this auxiliary form of representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Machine Translation[SMT]\n",
    "\n",
    "SMT works by training the translation engine with a very large volume of bilingual (source texts and their translations) and monolingual corpora. The system looks for statistical correlations between source texts and translations, both for entire segments and for shorter phrases within each segment, building a so-called translation model. It then generates confidence scores for how likely it is that a given source text will map to a translation. The translation engine itself has no notion of rules or grammar. SMT is the core of systems used by Google Translate and Bing Translator\n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "1. It requires very large and well-organized bilingual corpora for each language pair\n",
    "2. SMT engines fail when presented with texts that are not similar to material in the training corpora.Therefore, it is important to train the engine with texts that are similar to the material that will be translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Based Machine Translation\n",
    "\n",
    "In an EBMT system, a sentence is translated by analogy. A number of existing translation pairs of source and target sentences are used as examples. When a new source sentence is to be translated, the examples are retrieved to find similar ones in the source, then the target sentence is generated by imitating the translation of the matched examples. Because the hit rate for long sentences is very low, usually the examples and the source sentence are broken down into small fragments.\n",
    "\n",
    "**Diasdvantage**\n",
    "\n",
    "1. It requires large amount of examples for translation\n",
    "2. When there is no similar example found, the translation quality may be very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Neural machine translation (NMT) is based on the paradigm of machine learning and is the newest approach to MT. NMT uses neural networks that consist of nodes which can hold single words, phrases, or longer segments and relate to each other in a web of complex relationships based on bilingual texts used to train the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid  Machine Translation\n",
    "\n",
    "All the above methods have their shortcomings, and many hybrid MT approaches have been proposed. The two main categories of hybrid systems are:\n",
    "\n",
    "1. Rule-based engines using statistical translation for post processing and cleanup,\n",
    "2. Statistical systems guided by rule-based engines.\n",
    "3. Either of the above with some input from neural machine translation system.\n",
    "\n",
    "Almost all the practical MT systems adopt hybrid approaches to a certain extent, combining rule-based and statistical approaches. Most recently, more and more systems also take advantage of NMT to different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric for Machine Translation\n",
    "\n",
    "1. **Word error rate (WER)** is defined based on the distance between the system output and the reference translation at the word level.\n",
    "2. **Position-independent error rate (PER)** calculates the word error rate by treating each sentence as a bag of words and ignoring the word order.\n",
    "3. **Bilingual Evaluation Understudy (BLEU)** computes the n-gram precision rather than word error rate.\n",
    "4. **Metric for Evaluation of Translation with Explicit Ordering (METEOR)** takes stemming and synonyms into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Recurrent Neural Network has been the heart for Machine Translation.Different Architectures using RNN has been proposed over the years.In this section, we will learn some of the famous architectures,understand their working and later implement them from scratch in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Seq2Seq Model\n",
    "\n",
    "\n",
    "Research Paper -[Sequence to Sequence Learning with Neural Networks paper](https://arxiv.org/abs/1409.3215)<br>\n",
    "Implementation Code - [Colab link](colab link)\n",
    "\n",
    "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a context vector. You can think of the context vector as being an abstract representation of the entire input sentence. This vector is then decoded by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n",
    "\n",
    "![](images/encoder_decoder.png)\n",
    "\n",
    "**Encoder Sequence**\n",
    "At each time step,\n",
    "1. Input to the encoder \n",
    "    - current word, $x_t$ \n",
    "    - hidden state from the previous time-step, $h_{t-1}$\n",
    "2. Output from the encoder RNN\n",
    "    - a new hidden state $h_{t}$\n",
    "    \n",
    "We can represent the encoder as\n",
    "$$h_t = \\text{EncoderRNN}(x_t, h_{t-1})$$\n",
    " \n",
    "Once the final word, $x_T$, has been passed into the RNN, we use the final hidden state, $h_T$, as the context vector, i.e. $h_T = z$. This is a vector representation of the entire source sentence\n",
    "\n",
    "**Decoder Sequence**\n",
    " At each time-step,\n",
    " 1. Input to the decoder \n",
    "     - current word, $y_t$ \n",
    "     - the hidden state from the previous time-step, $s_{t-1}$ where the initial decoder hidden state, $s_0$, is the context vector, $s_0 = z = h_T$, i.e. the initial decoder hidden state is the final encoder hidden state\n",
    " 2. Output from the decoder\n",
    "     - $s_t$ to predict (by passing it through a Linear layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$.\n",
    "We can represent the decoder as \n",
    "$$s_t = \\text{DecoderRNN}(y_t, s_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "In this notebook, we'll visit all the concepts involved in implementing the model from the **Sequence to Sequence Learning with Neural Networks paper**.Refer this notebook as a lookup note to undersand any part of the implementation code.\n",
    "\n",
    "The model implementation is done using PyTorch and TorchText. This will be done on German to English translations, but the models can be applied to any problem that involves going from one sequence to another, such as summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [A STUDY OF MACHINE TRANSLATION METHODS AND THEIR CHALLENGES](https://www.ijarse.com/images/fullpdf/320.pdf)\n",
    "2. [Machine Translation-Introduction](https://www.andovar.com/machine-translation/)\n",
    "3. [Comparison_of_different_machine_translation_approaches](https://en.wikipedia.org/wiki/Comparison_of_different_machine_translation_approaches)\n",
    "4. [Develop Machine Learning Translation using Keras](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/)\n",
    "5. [Perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3)\n",
    "[cs124](https://web.stanford.edu/class/cs124/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
