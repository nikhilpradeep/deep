{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation [MT]\n",
    "\n",
    "***Machine Translation (MT)*** is a technology that automatically translates text using termbases and advanced grammatical, syntactic and semantic analysis techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Trasalation\n",
    "\n",
    "1. Rule Based Machine Translation[RBMT]\n",
    "2. Statistical Machine Translation[SMT]\n",
    "3. Example based Machine Translation [EBMT]\n",
    "4. Neural Machine Translation[NMT]\n",
    "5. Hybrid Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Machine Translation\n",
    "\n",
    "Also called Knowledge Based Machine Translation,it works by parsing a source sentence to identify words and analyze its structure, and then converting it into the target language based on a manually determined set of rules encoded by linguistic experts. The rules attempt to define correspondences between the structure of the source language and that of the target language.\n",
    "\n",
    "**Disadvantages**\n",
    "1. Development of an RBMT system is time-consuming and labor-intensive and may take several years for one language pair.\n",
    "2. Human-encoded rules are unable to cover all possible linguistic phenomena and conflicts between existing rules may lead to poor translation quality when facing real-life texts.RBMT engines donâ€™t deal well with slang or metaphorical texts\n",
    "\n",
    "There are three types of RBMT systems:\n",
    "\n",
    "**1. Direct Method (DIctionary Based Machine Translation)**\n",
    "<br>\n",
    "Source language text are translated without passing through an intermediary representation. The words will be\n",
    "translated as a dictionary does word by word, usually without much correlation of meaning between them\n",
    "\n",
    "**2 Transfer Rule Based Machine Translation** \n",
    "<br>\n",
    "Morphological and syntactical analysis is the fundamental approaches in Transfer based systems. Here source\n",
    "language text is converted into less language specific representation and same level of abstraction is generated\n",
    "with the help of grammar rules and bilingual dictionaries\n",
    "\n",
    "Eg- [Mantra](https://mantra-rajbhasha.rb-aai.in/) -A transfer based tool by Indian Govt\n",
    "\n",
    "**3 Interlingual RBMT Systems**\n",
    "<br>\n",
    " In this method, source language is translated into an intermediary representation which does not depends on any languages. Target language is derived from this auxiliary form of representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Machine Translation[SMT]\n",
    "\n",
    "SMT works by training the translation engine with a very large volume of bilingual (source texts and their translations) and monolingual corpora. The system looks for statistical correlations between source texts and translations, both for entire segments and for shorter phrases within each segment, building a so-called translation model. It then generates confidence scores for how likely it is that a given source text will map to a translation. The translation engine itself has no notion of rules or grammar. SMT is the core of systems used by Google Translate and Bing Translator\n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "1. It requires very large and well-organized bilingual corpora for each language pair\n",
    "2. SMT engines fail when presented with texts that are not similar to material in the training corpora.Therefore, it is important to train the engine with texts that are similar to the material that will be translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Based Machine Translation\n",
    "\n",
    "In an EBMT system, a sentence is translated by analogy. A number of existing translation pairs of source and target sentences are used as examples. When a new source sentence is to be translated, the examples are retrieved to find similar ones in the source, then the target sentence is generated by imitating the translation of the matched examples. Because the hit rate for long sentences is very low, usually the examples and the source sentence are broken down into small fragments.\n",
    "\n",
    "**Diasdvantage**\n",
    "\n",
    "1. It requires large amount of examples for translation\n",
    "2. When there is no similar example found, the translation quality may be very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Neural machine translation (NMT) is based on the paradigm of machine learning and is the newest approach to MT. NMT uses neural networks that consist of nodes which can hold single words, phrases, or longer segments and relate to each other in a web of complex relationships based on bilingual texts used to train the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid  Machine Translation\n",
    "\n",
    "All the above methods have their shortcomings, and many hybrid MT approaches have been proposed. The two main categories of hybrid systems are:\n",
    "\n",
    "1. Rule-based engines using statistical translation for post processing and cleanup,\n",
    "2. Statistical systems guided by rule-based engines.\n",
    "3. Either of the above with some input from neural machine translation system.\n",
    "\n",
    "Almost all the practical MT systems adopt hybrid approaches to a certain extent, combining rule-based and statistical approaches. Most recently, more and more systems also take advantage of NMT to different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric for Machine Translation\n",
    "\n",
    "1. **Word error rate (WER)** is defined based on the distance between the system output and the reference translation at the word level.\n",
    "2. **Position-independent error rate (PER)** calculates the word error rate by treating each sentence as a bag of words and ignoring the word order.\n",
    "3. **Bilingual Evaluation Understudy (BLEU)** computes the n-gram precision rather than word error rate.\n",
    "4. **Metric for Evaluation of Translation with Explicit Ordering (METEOR)** takes stemming and synonyms into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Recurrent Neural Network has been the heart for Machine Translation.Different Architectures using RNN has been proposed over the years.In this section, we will learn some of the famous architectures,understand their working and later implement them from scratch in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Seq2Seq Model\n",
    "\n",
    "\n",
    "Research Paper -[Sequence to Sequence Learning with Neural Networks paper](https://arxiv.org/abs/1409.3215)<br>\n",
    "Implementation Code - [Colab link](colab link)\n",
    "\n",
    "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a context vector. You can think of the context vector as being an abstract representation of the entire input sentence. This vector is then decoded by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n",
    "\n",
    "![](images/encoder_decoder.png)\n",
    "\n",
    "**Encoder Sequence**\n",
    "At each time step,\n",
    "1. Input to the encoder \n",
    "    - current word, $x_t$ \n",
    "    - hidden state from the previous time-step, $h_{t-1}$\n",
    "2. Output from the encoder RNN\n",
    "    - a new hidden state $h_{t}$\n",
    "    \n",
    "We can represent the encoder as\n",
    "$$h_t = \\text{EncoderRNN}(x_t, h_{t-1})$$\n",
    " \n",
    "Once the final word, $x_T$, has been passed into the RNN, we use the final hidden state, $h_T$, as the context vector, i.e. $h_T = z$. This is a vector representation of the entire source sentence\n",
    "\n",
    "**Decoder Sequence**\n",
    " At each time-step,\n",
    " 1. Input to the decoder \n",
    "     - current word, $y_t$ \n",
    "     - the hidden state from the previous time-step, $s_{t-1}$ where the initial decoder hidden state, $s_0$, is the context vector, $s_0 = z = h_T$, i.e. the initial decoder hidden state is the final encoder hidden state\n",
    " 2. Output from the decoder\n",
    "     - $s_t$ to predict (by passing it through a Linear layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$.\n",
    "We can represent the decoder as \n",
    "$$s_t = \\text{DecoderRNN}(y_t, s_{t-1})$$\n",
    "\n",
    "**Calculating loss**<br>\n",
    "Once we have our predicted target sentence, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, we compare it against our actual target sentence, $Y = \\{ y_1, y_2, ..., y_T \\}$, to calculate our loss. We then use this loss to update all of the parameters in our model.\n",
    "\n",
    "**Few things to note**\n",
    "\n",
    "1. We reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\"\n",
    "2. The initial hidden state, $h_0$,on the encoder side is usually either initialized to zeros or a learned parameter.\n",
    "3. We always use $<sos>$ for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\\hat{y}_{t-1}$. This is called teacher forcing\n",
    "4. When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference (i.e. real world usage) it is common to keep generating words until the model outputs an <eos> token or after a certain amount of words have been generated.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "**1. Objective**<br>\n",
    "We'll be building a machine learning model to go from once sequence to another, using PyTorch and TorchText. This will be done on German to English translations, but the models can be applied to any problem that involves going from one sequence to another, such as summarization.\n",
    "\n",
    "**2. Dataset**<br>\n",
    "The dataset we'll be using is the Multi30k dataset. This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence\n",
    "\n",
    "**3. Steps Followed**\n",
    "\n",
    "1. Create Tokenizers\n",
    "    - We use Spacy models to load English and German language so that we can access the tokeniers.\n",
    "    - We use Torch Text ***Fields*** for preprocessing our tokens.\n",
    "2. Create train_data,valid_data,test_data\n",
    "3. Building Vocabulary\n",
    "    - Using training data, we'll build the vocabulary for the source and target languages. The vocabulary is used to associate each unique token with an index (an integer) and this is used to build a one-hot encoding for each token (a vector of all zeros except for the position represented by the index, which is 1).We only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an <unk> (unknown) token\n",
    "4. Create train_iterator,valid_iterator,test_iterator\n",
    "    - Use iterators ,to return a batch of data which will have a src attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a trg attribute for passing to the model.Now the batch of data is represented as sequence of corresponding indexes, using the vocabulary.\n",
    "    - We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences.\n",
    "    \n",
    "5. Build the encoder,2 layer LSTM.The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut this down to 2-layers.\n",
    "6. Build the decoder, 2-layer LSTM instead of 4(in the paper)\n",
    "7. Build the seq2seq model.\n",
    "8. Training the seq2seq model.\n",
    "9. Evalaution of seq2seq model.\n",
    "\n",
    "\n",
    "**Encoder Architecture**\n",
    "\n",
    "![](images/encoder.png)\n",
    "\n",
    "For a multi-layer RNN, the input sentence, $X$, goes into the first (bottom) layer of the RNN and hidden states, $H=\\{h_1, h_2, ..., h_T\\}$, output by this layer are used as inputs to the RNN in the layer above. Thus, representing each layer with a superscript, the hidden states in the first layer are given by:\n",
    "\n",
    "$$h_t^1 = \\text{EncoderRNN}^1(x_t, h_{t-1}^1)$$\n",
    "The hidden states in the second layer are given by:\n",
    "\n",
    "$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$\n",
    "\n",
    "Using a multi-layer RNN also means we'll also need an initial hidden state as input per layer, $h_0^l$, and we will also output a context vector per layer, $z^l$.\n",
    "\n",
    "As we are using LSTM cells, which instead of just taking in a hidden state and returning a new hidden state per time-step, also take in and return a cell state, $c_t$, per time-step.\n",
    "$$\\begin{align*}\n",
    "        h_t= \\text{RNN}(x_t, h_{t-1})\\\\\n",
    "(h_t, c_t)= \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))\n",
    "\\end{align*}$$\n",
    "\n",
    "Extending our multi-layer equations to LSTMs, we get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "(h_t^1, c_t^1) &amp;= \\text{EncoderLSTM}^1(x_t, (h_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(h_t^2, c_t^2) &amp;= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "Note how only our hidden state from the first layer is passed as input to the second layer, and not the cell state.\n",
    "\n",
    "**Decoder Architecture**\n",
    "![](images/decoder.png)\n",
    "\n",
    "The first layer will receive a hidden and cell state from the previous time-step, $(s_{t-1}^1, c_{t-1}^1)$, and feed it through the LSTM with the current token, $y_t$, to produce a new hidden and cell state, $(s_t^1, c_t^1)$. The subsequent layers will use the hidden state from the layer below, $s_t^{l-1}$, and the previous hidden and cell states from their layer, $(s_{t-1}^l, c_{t-1}^l)$. This provides equations very similar to those in the encoder.\n",
    "\n",
    "$$\\begin{align*}\n",
    "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(y_t, (s_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "Remember that the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell states of our encoder from the same layer, i.e. $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
    "\n",
    "We then pass the hidden state from the top layer of the RNN, $s_t^L$, through a linear layer, $f$, to make a prediction of what the next token in the target (output) sequence should be, $\\hat{y}_{t+1}$.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(s_t^L)$$\n",
    "\n",
    "**Seq2Seq Model** -Full Model\n",
    "\n",
    "![](images/encoder_decoder_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [A STUDY OF MACHINE TRANSLATION METHODS AND THEIR CHALLENGES](https://www.ijarse.com/images/fullpdf/320.pdf)\n",
    "2. [Machine Translation-Introduction](https://www.andovar.com/machine-translation/)\n",
    "3. [Comparison_of_different_machine_translation_approaches](https://en.wikipedia.org/wiki/Comparison_of_different_machine_translation_approaches)\n",
    "4. [Develop Machine Learning Translation using Keras](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/)\n",
    "5. [Perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3)\n",
    "[cs124](https://web.stanford.edu/class/cs124/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
