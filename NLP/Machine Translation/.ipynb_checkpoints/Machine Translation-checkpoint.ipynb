{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation [MT]\n",
    "\n",
    "***Machine Translation (MT)*** is a technology that automatically translates text using termbases and advanced grammatical, syntactic and semantic analysis techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Trasalation\n",
    "\n",
    "1. Rule Based Machine Translation[RBMT]\n",
    "2. Statistical Machine Translation[SMT]\n",
    "3. Example based Machine Translation [EBMT]\n",
    "4. Neural Machine Translation[NMT]\n",
    "5. Hybrid Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Machine Translation\n",
    "\n",
    "Also called Knowledge Based Machine Translation,it works by parsing a source sentence to identify words and analyze its structure, and then converting it into the target language based on a manually determined set of rules encoded by linguistic experts. The rules attempt to define correspondences between the structure of the source language and that of the target language.\n",
    "\n",
    "**Disadvantages**\n",
    "1. Development of an RBMT system is time-consuming and labor-intensive and may take several years for one language pair.\n",
    "2. Human-encoded rules are unable to cover all possible linguistic phenomena and conflicts between existing rules may lead to poor translation quality when facing real-life texts.RBMT engines don’t deal well with slang or metaphorical texts\n",
    "\n",
    "There are three types of RBMT systems:\n",
    "\n",
    "**1. Direct Method (DIctionary Based Machine Translation)**\n",
    "<br>\n",
    "Source language text are translated without passing through an intermediary representation. The words will be\n",
    "translated as a dictionary does word by word, usually without much correlation of meaning between them\n",
    "\n",
    "**2 Transfer Rule Based Machine Translation** \n",
    "<br>\n",
    "Morphological and syntactical analysis is the fundamental approaches in Transfer based systems. Here source\n",
    "language text is converted into less language specific representation and same level of abstraction is generated\n",
    "with the help of grammar rules and bilingual dictionaries\n",
    "\n",
    "Eg- [Mantra](https://mantra-rajbhasha.rb-aai.in/) -A transfer based tool by Indian Govt\n",
    "\n",
    "**3 Interlingual RBMT Systems**\n",
    "<br>\n",
    " In this method, source language is translated into an intermediary representation which does not depends on any languages. Target language is derived from this auxiliary form of representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Machine Translation[SMT]\n",
    "\n",
    "SMT works by training the translation engine with a very large volume of bilingual (source texts and their translations) and monolingual corpora. The system looks for statistical correlations between source texts and translations, both for entire segments and for shorter phrases within each segment, building a so-called translation model. It then generates confidence scores for how likely it is that a given source text will map to a translation. The translation engine itself has no notion of rules or grammar. SMT is the core of systems used by Google Translate and Bing Translator\n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "1. It requires very large and well-organized bilingual corpora for each language pair\n",
    "2. SMT engines fail when presented with texts that are not similar to material in the training corpora.Therefore, it is important to train the engine with texts that are similar to the material that will be translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Based Machine Translation\n",
    "\n",
    "In an EBMT system, a sentence is translated by analogy. A number of existing translation pairs of source and target sentences are used as examples. When a new source sentence is to be translated, the examples are retrieved to find similar ones in the source, then the target sentence is generated by imitating the translation of the matched examples. Because the hit rate for long sentences is very low, usually the examples and the source sentence are broken down into small fragments.\n",
    "\n",
    "**Diasdvantage**\n",
    "\n",
    "1. It requires large amount of examples for translation\n",
    "2. When there is no similar example found, the translation quality may be very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Neural machine translation (NMT) is based on the paradigm of machine learning and is the newest approach to MT. NMT uses neural networks that consist of nodes which can hold single words, phrases, or longer segments and relate to each other in a web of complex relationships based on bilingual texts used to train the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid  Machine Translation\n",
    "\n",
    "All the above methods have their shortcomings, and many hybrid MT approaches have been proposed. The two main categories of hybrid systems are:\n",
    "\n",
    "1. Rule-based engines using statistical translation for post processing and cleanup,\n",
    "2. Statistical systems guided by rule-based engines.\n",
    "3. Either of the above with some input from neural machine translation system.\n",
    "\n",
    "Almost all the practical MT systems adopt hybrid approaches to a certain extent, combining rule-based and statistical approaches. Most recently, more and more systems also take advantage of NMT to different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Machine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric for Machine Translation\n",
    "\n",
    "1. **Word error rate (WER)** is defined based on the distance between the system output and the reference translation at the word level.\n",
    "2. **Position-independent error rate (PER)** calculates the word error rate by treating each sentence as a bag of words and ignoring the word order.\n",
    "3. **Bilingual Evaluation Understudy (BLEU)** computes the n-gram precision rather than word error rate.\n",
    "4. **Metric for Evaluation of Translation with Explicit Ordering (METEOR)** takes stemming and synonyms into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Different Architectures using different neural networks has been proposed over the years.In this section, we will learn some of the famous architectures,understand their working and later implement them from scratch in Pytorch.\n",
    "\n",
    "1. Seq2Seq Model\n",
    "\n",
    "2. Seq2Seq with Attention\n",
    "\n",
    "3. Convolution Based Models\n",
    "\n",
    "4. Transformer (dispensing with recurrence and convolutions entirely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq [Encoder - Decoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecure \n",
    "\n",
    "Research Paper -[Sequence to Sequence Learning with Neural Networks paper](https://arxiv.org/abs/1409.3215)<br>\n",
    "Implementation Code - [Colab link](colab link)\n",
    "\n",
    "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a context vector. You can think of the context vector as being an abstract representation of the entire input sentence. This vector is then decoded by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n",
    "\n",
    "![](images/encoder_decoder.png)\n",
    "\n",
    "**Encoder Sequence**\n",
    "At each time step,\n",
    "1. Input to the encoder \n",
    "    - current word, $x_t$ \n",
    "    - hidden state from the previous time-step, $h_{t-1}$\n",
    "2. Output from the encoder RNN\n",
    "    - a new hidden state $h_{t}$\n",
    "    \n",
    "We can represent the encoder as\n",
    "$$h_t = \\text{EncoderRNN}(x_t, h_{t-1})$$\n",
    " \n",
    "Once the final word, $x_T$, has been passed into the RNN, we use the final hidden state, $h_T$, as the context vector, i.e. $h_T = z$. This is a vector representation of the entire source sentence\n",
    "\n",
    "**Decoder Sequence**\n",
    " At each time-step,\n",
    " 1. Input to the decoder \n",
    "     - current word, $y_t$ \n",
    "     - the hidden state from the previous time-step, $s_{t-1}$ where the initial decoder hidden state, $s_0$, is the context vector, $s_0 = z = h_T$, i.e. the initial decoder hidden state is the final encoder hidden state\n",
    " 2. Output from the decoder\n",
    "     - $s_t$ to predict (by passing it through a Linear layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$.\n",
    "We can represent the decoder as \n",
    "$$s_t = \\text{DecoderRNN}(y_t, s_{t-1})$$\n",
    "\n",
    "**Calculating loss**<br>\n",
    "Once we have our predicted target sentence, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, we compare it against our actual target sentence, $Y = \\{ y_1, y_2, ..., y_T \\}$, to calculate our loss. We then use this loss to update all of the parameters in our model.\n",
    "\n",
    "**Few things to note**\n",
    "\n",
    "1. We reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\"\n",
    "2. The initial hidden state, $h_0$,on the encoder side is usually either initialized to zeros or a learned parameter.\n",
    "3. We always use $<sos>$ for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\\hat{y}_{t-1}$. This is called teacher forcing\n",
    "4. When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference (i.e. real world usage) it is common to keep generating words until the model outputs an <eos> token or after a certain amount of words have been generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "**1. Objective**<br>\n",
    "We'll be building a machine learning model to go from once sequence to another, using PyTorch and TorchText. This will be done on German to English translations, but the models can be applied to any problem that involves going from one sequence to another, such as summarization.\n",
    "\n",
    "**2. Dataset**<br>\n",
    "The dataset we'll be using is the Multi30k dataset. This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence\n",
    "\n",
    "**3. Steps Followed**\n",
    "\n",
    "1. Create Tokenizers\n",
    "    - We use Spacy models to load English and German language so that we can access the tokeniers.\n",
    "    - We use Torch Text ***Fields*** for preprocessing our tokens.\n",
    "2. Create train_data,valid_data,test_data\n",
    "3. Building Vocabulary\n",
    "    - Using training data, we'll build the vocabulary for the source and target languages. The vocabulary is used to associate each unique token with an index (an integer) and this is used to build a one-hot encoding for each token (a vector of all zeros except for the position represented by the index, which is 1).We only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an <unk> (unknown) token\n",
    "4. Create train_iterator,valid_iterator,test_iterator\n",
    "    - Use iterators ,to return a batch of data which will have a src attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a trg attribute for passing to the model.Now the batch of data is represented as sequence of corresponding indexes, using the vocabulary.\n",
    "    - We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences.\n",
    "    \n",
    "5. Build the encoder,2 layer LSTM.The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut this down to 2-layers.\n",
    "6. Build the decoder, 2-layer LSTM instead of 4(in the paper)\n",
    "7. Build the seq2seq model.\n",
    "8. Training the seq2seq model.\n",
    "9. Evalaution of seq2seq model.\n",
    "\n",
    "\n",
    "**Encoder Architecture**\n",
    "\n",
    "![](images/encoder.png)\n",
    "\n",
    "For a multi-layer RNN, the input sentence, $X$, goes into the first (bottom) layer of the RNN and hidden states, $H=\\{h_1, h_2, ..., h_T\\}$, output by this layer are used as inputs to the RNN in the layer above. Thus, representing each layer with a superscript, the hidden states in the first layer are given by:\n",
    "\n",
    "$$h_t^1 = \\text{EncoderRNN}^1(x_t, h_{t-1}^1)$$\n",
    "The hidden states in the second layer are given by:\n",
    "\n",
    "$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$\n",
    "\n",
    "Using a multi-layer RNN also means we'll also need an initial hidden state as input per layer, $h_0^l$, and we will also output a context vector per layer, $z^l$.\n",
    "\n",
    "As we are using LSTM cells, which instead of just taking in a hidden state and returning a new hidden state per time-step, also take in and return a cell state, $c_t$, per time-step.\n",
    "$$\\begin{align*}\n",
    "        h_t= \\text{RNN}(x_t, h_{t-1})\\\\\n",
    "(h_t, c_t)= \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))\n",
    "\\end{align*}$$\n",
    "\n",
    "Extending our multi-layer equations to LSTMs, we get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "(h_t^1, c_t^1) &amp;= \\text{EncoderLSTM}^1(x_t, (h_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(h_t^2, c_t^2) &amp;= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "Note how only our hidden state from the first layer is passed as input to the second layer, and not the cell state.\n",
    "\n",
    "**Decoder Architecture**\n",
    "![](images/decoder.png)\n",
    "\n",
    "The first layer will receive a hidden and cell state from the previous time-step, $(s_{t-1}^1, c_{t-1}^1)$, and feed it through the LSTM with the current token, $y_t$, to produce a new hidden and cell state, $(s_t^1, c_t^1)$. The subsequent layers will use the hidden state from the layer below, $s_t^{l-1}$, and the previous hidden and cell states from their layer, $(s_{t-1}^l, c_{t-1}^l)$. This provides equations very similar to those in the encoder.\n",
    "\n",
    "$$\\begin{align*}\n",
    "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(y_t, (s_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "Remember that the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell states of our encoder from the same layer, i.e. $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
    "\n",
    "We then pass the hidden state from the top layer of the RNN, $s_t^L$, through a linear layer, $f$, to make a prediction of what the next token in the target (output) sequence should be, $\\hat{y}_{t+1}$.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(s_t^L)$$\n",
    "\n",
    "**Seq2Seq Model** -Full Model\n",
    "\n",
    "![](images/encoder_decoder_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "1. The fixed size of the context matrix passed from the encoder to the decoder is a bottleneck.\n",
    "2. Difficulty of encoding long sequences and recalling long-term dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-Encoder Decoder for Statistical Machine Translation\n",
    "\n",
    "Research Paper<br>\n",
    "[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)<br>\n",
    "[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)<br>\n",
    "Colab Link - [](link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Details\n",
    "\n",
    "**1. Encoder**\n",
    "\n",
    "1. ENcoder uses single layer GRU,instead of multi-layer LSTM used in vanila Encoder -Decoder Seq2Seq model,.<br>\n",
    "       GRU only requires and returns a hidden state, there is no cell state like in the LSTM.\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t = \\text{GRU}(x_t, h_{t-1})\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Input sequence- $X = \\{x_1, x_2, ... , x_T\\}$<br>\n",
    "Hidden states, $H = \\{h_1, h_2, ..., h_T\\}$, <br> \n",
    "Context vector (the final hidden state), $z=h_T$ <br>\n",
    "![](images/encoder_gru.png)\n",
    "\n",
    "**2. Decoder**<br>\n",
    "1. GRU in the decoder now not only takes the target token, $y_t$ and the previous hidden state $s_{t-1}$ as inputs, but also takes the context vector $z$.\n",
    "$$s_t = \\text{DecoderGRU}(y_t, s_{t-1}, z)$$\n",
    "2. Linear layer takes the current token, $\\hat{y}_t$ and the context vector, $z$ along with  top-layer decoder hidden state at that time-step, $s_t$,  as the input.\n",
    "$$\\hat{y}_{t+1} = f(y_t, s_t, z)$$\n",
    "\n",
    "![](images/decoder_gru.png)\n",
    "\n",
    "What are the inputs to the first generating token???\n",
    "\n",
    "**Seq2Seq Model** \n",
    "![](images/encoder_decoder_gru.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq with Attention\n",
    "\n",
    "\n",
    "The most important distinguishing feature of this approach from the basic encoder–decoder is that\n",
    "it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes\n",
    "the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively\n",
    "while decoding the translation.\n",
    "\n",
    "1. Additive Attention-[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "2. Additive and Multiplicative Attention (https://arxiv.org/abs/1508.04025)\n",
    "\n",
    "![](images/attention_eq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Attention\n",
    "\n",
    "It is an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated(**Attention**). The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\n",
    "\n",
    "\n",
    "**Attention** works by first, calculating an attention vector, $a$, that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states, $H$, to get a weighted source vector, $w$.\n",
    "\n",
    "$$w = \\sum_{i}a_ih_i$$\n",
    "We calculate a new weighted source vector every time-step when decoding, using it as input to our decoder RNN as well as the linear layer to make a prediction. \n",
    "\n",
    "We'll explain how to do all of this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture Details\n",
    "\n",
    "**1. Encoder**\n",
    "\n",
    "1. We use single layer bidirectional RNN.With a bidirectional RNN, we have two RNNs in each layer. A forward RNN going over the sentence from left to right (shown below in green), and a backward RNN going over the sentence from right to left (yellow)\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t^\\rightarrow = \\text{EncoderGRU}^\\rightarrow(x_t^\\rightarrow,h_t^\\rightarrow)\\\\\n",
    "h_t^\\leftarrow= \\text{EncoderGRU}^\\leftarrow(x_t^\\leftarrow,h_t^\\leftarrow)\n",
    "\\end{align*}$$\n",
    "\n",
    "Input sequence- $X = \\{x_1, x_2, ... , x_T\\}$<br>\n",
    "Hidden states<br>\n",
    "$H^\\rightarrow = \\{h_1^\\rightarrow, h_2^\\rightarrow, ..., h_T^\\rightarrow\\}$, <br> \n",
    "$H^\\leftarrow = \\{h_1^\\leftarrow, h_2^\\leftarrow, ..., h_T^\\leftarrow\\}$, <br> \n",
    "Two Context vectors(the final hidden state) ***but they are not the final context vector that will be fed into Decoder.We will be using Attention mechanism to calculate the context vector for decoder*** :<br>\n",
    "    One from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$<BR>\n",
    " One from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$\n",
    "  \n",
    "![](images/encoder_bidir.png)  \n",
    "\n",
    "\n",
    "**2. Attention**<br>\n",
    "Intuitively,this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$.\n",
    "\n",
    "![](images/attention_eq.png)\n",
    "\n",
    "In the equation above :<br>\n",
    "$h_{t}$ - Previous hidden state of decoder<br>\n",
    "$h_{s}$ - All the hidden states of Encoder<br>\n",
    "\n",
    "Working on the 2nd equation<br>\n",
    "**1. Calculate the energy, $E_t$**\n",
    "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$\n",
    "1. Take the encoder hidden states and concat them together,represented as **H** in the equation of size [No of hidden units in encoder(n),No of elements in input sequence(t)]\n",
    "2. Take the previous decoder hidden state,**s<sub>t-1</sub>** of size  [No of hidden units in decoder(n) , 1]\n",
    "3. Concat them together and pass them through a linear layer **(attn)** and a **$\\tanh$** activation function\n",
    "    \n",
    "**2. Multiplying with $v_a^{T}$**    \n",
    "    The ouput is of size [No of hidden units in decoder(n),No of elements in input sequence(t)].However for each example in the batch ,the attention should be over the length of the source sentence.Hence we mulitpy it with $v_a^{T}$ of size [No of hidden units in decoder(n),1].\n",
    "\n",
    "**3. Applying softmax**<br>\n",
    "We then pass the attention vector through **softmax** layer to make value between 0 and 1.This gives us the attention over the source sentence.\n",
    "\n",
    "**3. Decoder**\n",
    "1. Use this attention vector to create a weighted source vector, $w_t$, denoted by weighted, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n",
    "\n",
    "$$w_t = a_t H$$\n",
    "2. The input word (that has been embedded), $y_t$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $y_t$ and $w_t$ being concatenated together.\n",
    "\n",
    "$$s_t = \\text{DecoderGRU}(y_t, w_t, s_{t-1})$$\n",
    "\n",
    "3. We then pass $y_t$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(y_t, w_t, s_t)$$\n",
    "\n",
    "![](images/decoder_bidir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "**1. Sequential Nature of RNN**<br>\n",
    "When we process a sequence using RNNs, each hidden state depends on the previous hidden state. This becomes a major pain point on GPUs: GPUs have a lot of computational capability and they hate having to wait for data to become available. Even with technologies like CuDNN, RNNs are painfully inefficient and slow on the GPU.\n",
    "\n",
    "**2. Long Range Dependencies**<br>\n",
    "RNN based architectures finds difficulty in learning long-range dependencies within the input and output sequences.\n",
    "In essence, there are three kinds of dependencies in neural machine translations: dependencies between\n",
    "1. the input and output tokens\n",
    "2. the input tokens themselves\n",
    "3. the output tokens themselves.\n",
    "\n",
    "The traditional attention mechanism largely solved the first dependency by giving the decoder access to the entire input sequence.**Transformer** extend this mechanism to the processing input and output sentences as well by  allowing the encoder and decoder to see the entire input sequence all at once, directly modeling these dependencies using attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Details\n",
    "![](images/transformer_Arch.png)\n",
    "\n",
    "\n",
    "Each sub-layer (self-attention, ffnn) in each encoder and decoder has a residual connection around it, and is followed by a layer-normalization step.\n",
    "\n",
    "Let us try to uncover each module one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Stack\n",
    "\n",
    "The encoding component is a stack of encoders (the paper stacks six of them on top of each other.The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\n",
    "1. Multi Head Attention - a refined version of Self Attention\n",
    "2. Feed Forward Neural Network\n",
    "\n",
    "\n",
    "![](images/encoder_transformer.png)\n",
    "\n",
    "\n",
    "**1. Sef Attention**\n",
    "\n",
    " Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n",
    " ![](images/self_attention.png)\n",
    " As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\n",
    " \n",
    " Steps carried out in the Encoder Stack\n",
    " \n",
    " 1. For each encoder’s input vectors (in this case, the embedding of each word),we **create a Query vector, a Key vector, and a Value vector**.These vectors are created by multiplying the embedding by three matrices $$W_{q},W_{k},W_{v}$$ that we trained during the training process.\n",
    " 2. **Calculating the self-attention score for each word of the input sentence against the current input vector**. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.The score is calculated by taking the **dot product of the query vector with the key vector of the respective word we’re scoring**.\n",
    " 3. **For stable gradients**,divide the score by 8.(the square root of the dimension of the key vectors used in the paper – 64).\n",
    " 4. **Pass the values through softmax** to cap the values between 0 and 1.The softmax score determines how much each word will be expressed at this position.\n",
    " 5. **Multiply each value vector by the softmax score** (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
    " 6. **Sum up the weighted value vectors**. This produces the output of the self-attention layer at this position (for the first word).\n",
    " \n",
    "![](images/multi_head_attention.png)\n",
    "\n",
    "**Multi Head Attention**<br>\n",
    "With Multi-Head Attention,we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder).\n",
    " \n",
    "That means,If we do the same self-attention calculation we outlined above just eight different times with different weight matrices, we end up with eight different attention scores.\n",
    "7. Concatenate the resulting attention scores.\n",
    "\n",
    "**Feed Forward Network**<br>\n",
    "8. Pass the concatenated attention scores to a linear layer FFN to produce the output of the layer i.e  multiply concatenated attention scores with the  Weight Matrix W<sub>O</sub>\n",
    "\n",
    "![](images/multi_head_attention1.png)\n",
    "Here h=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Decoding Stack\n",
    "\n",
    "Similar to the encoder stack,decoding part has a stack of decoders (the paper stacks six of them on top of each other.The decoders are all identical in structure (yet they do not share weights). Each one is broken down into three sub-layers:\n",
    "1. Multi Head Attention[decoder] \n",
    "2. Multi-head attention over the output of the encoder stack [from encoder]\n",
    "2. Feed Forward Neural Network\n",
    "\n",
    "![](images/decoder_attention.png)\n",
    "\n",
    "**Steps carried out in Decoding Stack**\n",
    "\n",
    "1. The output vector of each step is fed to the bottom decoder.\n",
    "2. Multi head Attention[decoder] layer calculates the attention score.\n",
    "***Note-the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "3. The output of the top encoder is then transformed into a set of attention vectors K and V.and fed in the multi head attention.\n",
    "4. Multi Head attention[from encoder] creates its Queries matrix from the layer below it, and  Keys and Values matrix from the output of the encoder stack.\n",
    "5. Decoder stack output the score .\n",
    "6. The score is fed to a linear layer to produce output equal to the size of the vacabulary.\n",
    "7. The score is then turned into probabilities using softmax.The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
    "8. The output for this time step is fed to the bottom decoder in the next time step.\n",
    "9. The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output.\n",
    "\n",
    "#### Other things to note in the architecture**\n",
    "\n",
    "\n",
    "2. Loss Function -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects\n",
    "\n",
    "1. Machine Translation\n",
    "2. Text Summarization \n",
    "3. Questioning Answering \n",
    "4. Chatbot etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "https://github.com/tensorflow/tensor2tensor#suggested-datasets-and-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [A STUDY OF MACHINE TRANSLATION METHODS AND THEIR CHALLENGES](https://www.ijarse.com/images/fullpdf/320.pdf)\n",
    "2. [Machine Translation-Introduction](https://www.andovar.com/machine-translation/)\n",
    "3. [Comparison_of_different_machine_translation_approaches](https://en.wikipedia.org/wiki/Comparison_of_different_machine_translation_approaches)\n",
    "4. [Develop Machine Learning Translation using Keras](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/)\n",
    "5. [Perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3)\n",
    "[cs124](https://web.stanford.edu/class/cs124/)\n",
    "6. [Google Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "7. [Attention is All you need-Video](https://www.youtube.com/watch?v=rBCqOTEfxvg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
